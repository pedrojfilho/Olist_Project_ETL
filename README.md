<h1 align="center">ğŸ› ï¸ Olist Data Engineering Project â€” AWS Medallion Architecture</h1>

<p align="center">
  <strong>End-to-end Data Lakehouse built with PySpark, AWS S3, and the Medallion Architecture.</strong>
</p>

---

# ğŸ“Œ Overview

This project implements a full **Data Engineering pipeline** using the Olist e-commerce dataset and the **Medallion Architecture (Raw â†’ Bronze â†’ Silver â†’ Gold)** on AWS S3.

It simulates a real production scenario, including:

- Data ingestion from CSV (Raw)
- Transformation and standardization (Bronze)
- Cleaning and enrichment (Silver)
- Analytical modeling with Star Schema (Gold)
- JSON schema versioning for all layers
- Protected Git workflow with Pull Requests

---

# ğŸ—ï¸ Architecture

## ğŸ”· Medallion Architecture

![Medallion Flow](architecture/medallion_flow.png)

### ğŸŸ« RAW
Stores original CSV files exactly as received, without transformation.

### ğŸŸ§ BRONZE
Structured and standardized data, stored in Parquet format.

### âšª SILVER
Clean, enriched, analytics-ready data.

### ğŸŸ¨ GOLD
Business-level tables in a **Star Schema** for BI tools and analytics.

---

# â­ Star Schema (Gold Layer)

![Star Schema](architecture/star_schema.png)

The Gold layer includes:

- Fact Sales
- Dim Customer
- Dim Product
- Dim Seller
- Dim Date
- Dim Geolocation

---

# ğŸ“ Repository Structure

```md
OLIST_PROJECT_ETL_AWS/
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ medallion_overview.md
â”‚   â”œâ”€â”€ medallion.png
â”‚   â””â”€â”€ star_schema.png
â”‚
â”œâ”€â”€ RAW/
â”‚   â”œâ”€â”€ Notebooks/
â”‚   â”‚   â”œâ”€â”€ check_schemas.ipynb
â”‚   â”‚   â””â”€â”€ sample_schemas/
â”‚   â”‚       â”œâ”€â”€ category_schema.json
â”‚   â”‚       â”œâ”€â”€ customer_schema.json
â”‚   â”‚       â”œâ”€â”€ geolocation_schema.json
â”‚   â”‚       â”œâ”€â”€ items_schema.json
â”‚   â”‚       â”œâ”€â”€ orders_schema.json
â”‚   â”‚       â”œâ”€â”€ payments_schema.json
â”‚   â”‚       â”œâ”€â”€ products_schema.json
â”‚   â”‚       â””â”€â”€ sellers_schema.json
â”‚
â”œâ”€â”€ venv/
â”‚
â””â”€â”€ README.md


---

# ğŸ§° Technologies Used

| Technology | Purpose |
|-----------|---------|
| AWS S3 | Data Lake storage |
| PySpark | ETL processing and schema enforcement |
| Parquet | Optimized columnar storage |
| AWS CLI | Authentication and access |
| Python | ETL logic and orchestration |
| VSCode | Local development |
| GitHub | Versioning and PR workflow |

---

# âš™ï¸ Running the Project Locally

##  Initialize Spark with S3 Access

# This project uses protected main branch + Pull Request workflow.

Branch strategy:

main                  â†’ stable production branch
feature/raw-layer     â†’ Raw ingestion development
feature/bronze-layer  â†’ Bronze transformations
feature/silver-layer  â†’ Silver cleaning & enrichment
feature/gold-layer    â†’ Star schema modeling
docs                  â†’ Documentation updates

Requirements:

- Pull Request is required to merge into main

- Force push is blocked

- Code review required

- Conversation threads must be resolved

# ğŸ“ˆ Roadmap

- [x] Setup AWS & Spark environment  
- [x] Load RAW CSV data from S3  
- [x] Generate JSON schemas  
- [ ] Implement Bronze transformations  
- [ ] Build Silver layer cleaning  
- [ ] Create Gold fact & dimension tables  
- [ ] Add Athena/Glue catalog integration  
- [ ] Build dashboards for analytics  

ğŸ§‘â€ğŸ’» Author

Pedro Filho â€” Data Engineering Project (AWS + PySpark + Medallion Architecture)