import sys
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext
import pyspark.sql.functions as F

args = getResolvedOptions(sys.argv, ["JOB_NAME"])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

fact_orders = spark.read.parquet("s3://pedro-datalake-project/gold/fact_orders/")
sellers_dim = spark.read.parquet("s3://pedro-datalake-project/gold/dim_sellers/")

fact_orders = fact_orders.drop("seller_city", "seller_state", "region")

df = fact_orders.alias("f").join(
    sellers_dim.select(
        "seller_id",
        "seller_city",
        "seller_state",
        "region"
    ).alias("d"),
    "seller_id",
    "left"
)

df_seller = (
    df.groupBy("seller_id", "seller_city", "seller_state", "region")
      .agg(
          F.count("order_id").alias("total_orders"),
          F.countDistinct("customer_id").alias("total_customers"),
          F.sum("price").alias("total_revenue"),
          F.sum("freight_value").alias("total_freight"),
          F.avg("delivery_days").alias("avg_delivery_time"),
          F.sum(F.when(F.col("is_delayed"), 1).otherwise(0)).alias("late_deliveries"),
          F.avg("price").alias("avg_order_value")
      )
)

df_seller = df_seller.withColumn(
    "on_time_rate",
    1 - (F.col("late_deliveries") / F.col("total_orders"))
).withColumn("created_at", F.current_timestamp())

output = "s3://pedro-datalake-project/gold/fact_seller_performance/"
df_seller.write.mode("overwrite").parquet(output)

job.commit()
